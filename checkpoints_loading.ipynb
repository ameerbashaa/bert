{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/deepmipt/bert.git@feat/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_dp import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = bert.BERT(name='bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {'pos_ids': tf.constant([[0, 1]]), 'segment_ids': tf.constant([[0, 0]]), 'token_ids': tf.constant([[1, 4]])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = tf.constant([[1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/ops/embedding_ops.py:132: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=4165, shape=(1, 2, 768), dtype=float32, numpy=\n",
       "array([[[-2.2176013 ,  1.0577515 ,  0.84728235, ..., -1.1046779 ,\n",
       "          1.3490838 , -1.1778777 ],\n",
       "        [-2.1938188 ,  1.071013  ,  0.88197637, ..., -1.1184927 ,\n",
       "          1.3465396 , -1.1005738 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model(inputs=inputs, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embeddings (BERTCombinedEmbe multiple                  92206848  \n",
      "_________________________________________________________________\n",
      "embeddings/dropout (Dropout) multiple                  0         \n",
      "_________________________________________________________________\n",
      "embeddings/LayerNorm (LayerN multiple                  1536      \n",
      "_________________________________________________________________\n",
      "encoder (TransformerEncoder) multiple                  0         \n",
      "_________________________________________________________________\n",
      "pooler/dense (Dense)         multiple                  590592    \n",
      "=================================================================\n",
      "Total params: 92,798,976\n",
      "Trainable params: 92,798,976\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/bert'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.get_file(\n",
    "    fname='bert',\n",
    "    origin='http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz',\n",
    "    cache_subdir='models',\n",
    "    extract=True,\n",
    "    cache_dir='.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_checkpoint = 'models/rubert_cased_L-12_H-768_A-12_v1/bert_model.ckpt'\n",
    "var_names = tf.train.list_variables(init_checkpoint)\n",
    "check_point = tf.train.load_checkpoint(init_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_vars = set()\n",
    "for v, _ in var_names:\n",
    "    checkpoint_vars.add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert/embeddings/LayerNorm/beta',\n",
       " 'bert/embeddings/LayerNorm/gamma',\n",
       " 'bert/embeddings/position_embeddings',\n",
       " 'bert/embeddings/position_embeddings/AdamWeightDecayOptimizer',\n",
       " 'bert/embeddings/position_embeddings/AdamWeightDecayOptimizer_1',\n",
       " 'bert/embeddings/token_type_embeddings',\n",
       " 'bert/embeddings/word_embeddings',\n",
       " 'bert/encoder/layer_0/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_0/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_0/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_0/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_0/attention/self/key/bias',\n",
       " 'bert/encoder/layer_0/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_0/attention/self/query/bias',\n",
       " 'bert/encoder/layer_0/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_0/attention/self/value/bias',\n",
       " 'bert/encoder/layer_0/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_0/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_0/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_0/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_0/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_0/output/dense/bias',\n",
       " 'bert/encoder/layer_0/output/dense/kernel',\n",
       " 'bert/encoder/layer_1/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_1/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_1/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_1/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_1/attention/self/key/bias',\n",
       " 'bert/encoder/layer_1/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_1/attention/self/query/bias',\n",
       " 'bert/encoder/layer_1/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_1/attention/self/value/bias',\n",
       " 'bert/encoder/layer_1/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_1/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_1/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_1/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_1/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_1/output/dense/bias',\n",
       " 'bert/encoder/layer_1/output/dense/kernel',\n",
       " 'bert/encoder/layer_10/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_10/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_10/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_10/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_10/attention/self/key/bias',\n",
       " 'bert/encoder/layer_10/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_10/attention/self/query/bias',\n",
       " 'bert/encoder/layer_10/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_10/attention/self/value/bias',\n",
       " 'bert/encoder/layer_10/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_10/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_10/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_10/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_10/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_10/output/dense/bias',\n",
       " 'bert/encoder/layer_10/output/dense/kernel',\n",
       " 'bert/encoder/layer_11/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_11/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_11/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_11/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_11/attention/self/key/bias',\n",
       " 'bert/encoder/layer_11/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_11/attention/self/query/bias',\n",
       " 'bert/encoder/layer_11/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_11/attention/self/value/bias',\n",
       " 'bert/encoder/layer_11/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_11/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_11/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_11/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_11/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_11/output/dense/bias',\n",
       " 'bert/encoder/layer_11/output/dense/kernel',\n",
       " 'bert/encoder/layer_2/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_2/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_2/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_2/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_2/attention/self/key/bias',\n",
       " 'bert/encoder/layer_2/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_2/attention/self/query/bias',\n",
       " 'bert/encoder/layer_2/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_2/attention/self/value/bias',\n",
       " 'bert/encoder/layer_2/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_2/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_2/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_2/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_2/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_2/output/dense/bias',\n",
       " 'bert/encoder/layer_2/output/dense/kernel',\n",
       " 'bert/encoder/layer_3/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_3/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_3/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_3/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_3/attention/self/key/bias',\n",
       " 'bert/encoder/layer_3/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_3/attention/self/query/bias',\n",
       " 'bert/encoder/layer_3/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_3/attention/self/value/bias',\n",
       " 'bert/encoder/layer_3/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_3/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_3/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_3/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_3/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_3/output/dense/bias',\n",
       " 'bert/encoder/layer_3/output/dense/kernel',\n",
       " 'bert/encoder/layer_4/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_4/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_4/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_4/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_4/attention/self/key/bias',\n",
       " 'bert/encoder/layer_4/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_4/attention/self/query/bias',\n",
       " 'bert/encoder/layer_4/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_4/attention/self/value/bias',\n",
       " 'bert/encoder/layer_4/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_4/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_4/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_4/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_4/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_4/output/dense/bias',\n",
       " 'bert/encoder/layer_4/output/dense/kernel',\n",
       " 'bert/encoder/layer_5/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_5/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_5/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_5/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_5/attention/self/key/bias',\n",
       " 'bert/encoder/layer_5/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_5/attention/self/query/bias',\n",
       " 'bert/encoder/layer_5/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_5/attention/self/value/bias',\n",
       " 'bert/encoder/layer_5/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_5/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_5/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_5/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_5/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_5/output/dense/bias',\n",
       " 'bert/encoder/layer_5/output/dense/kernel',\n",
       " 'bert/encoder/layer_6/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_6/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_6/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_6/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_6/attention/self/key/bias',\n",
       " 'bert/encoder/layer_6/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_6/attention/self/query/bias',\n",
       " 'bert/encoder/layer_6/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_6/attention/self/value/bias',\n",
       " 'bert/encoder/layer_6/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_6/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_6/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_6/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_6/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_6/output/dense/bias',\n",
       " 'bert/encoder/layer_6/output/dense/kernel',\n",
       " 'bert/encoder/layer_7/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_7/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_7/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_7/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_7/attention/self/key/bias',\n",
       " 'bert/encoder/layer_7/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_7/attention/self/query/bias',\n",
       " 'bert/encoder/layer_7/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_7/attention/self/value/bias',\n",
       " 'bert/encoder/layer_7/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_7/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_7/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_7/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_7/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_7/output/dense/bias',\n",
       " 'bert/encoder/layer_7/output/dense/kernel',\n",
       " 'bert/encoder/layer_8/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_8/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_8/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_8/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_8/attention/self/key/bias',\n",
       " 'bert/encoder/layer_8/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_8/attention/self/query/bias',\n",
       " 'bert/encoder/layer_8/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_8/attention/self/value/bias',\n",
       " 'bert/encoder/layer_8/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_8/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_8/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_8/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_8/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_8/output/dense/bias',\n",
       " 'bert/encoder/layer_8/output/dense/kernel',\n",
       " 'bert/encoder/layer_9/attention/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_9/attention/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_9/attention/output/dense/bias',\n",
       " 'bert/encoder/layer_9/attention/output/dense/kernel',\n",
       " 'bert/encoder/layer_9/attention/self/key/bias',\n",
       " 'bert/encoder/layer_9/attention/self/key/kernel',\n",
       " 'bert/encoder/layer_9/attention/self/query/bias',\n",
       " 'bert/encoder/layer_9/attention/self/query/kernel',\n",
       " 'bert/encoder/layer_9/attention/self/value/bias',\n",
       " 'bert/encoder/layer_9/attention/self/value/kernel',\n",
       " 'bert/encoder/layer_9/intermediate/dense/bias',\n",
       " 'bert/encoder/layer_9/intermediate/dense/kernel',\n",
       " 'bert/encoder/layer_9/output/LayerNorm/beta',\n",
       " 'bert/encoder/layer_9/output/LayerNorm/gamma',\n",
       " 'bert/encoder/layer_9/output/dense/bias',\n",
       " 'bert/encoder/layer_9/output/dense/kernel',\n",
       " 'bert/pooler/dense/bias',\n",
       " 'bert/pooler/dense/kernel',\n",
       " 'cls/predictions/output_bias',\n",
       " 'cls/predictions/transform/LayerNorm/beta',\n",
       " 'cls/predictions/transform/LayerNorm/gamma',\n",
       " 'cls/predictions/transform/dense/bias',\n",
       " 'cls/predictions/transform/dense/kernel',\n",
       " 'cls/seq_relationship/output_bias',\n",
       " 'cls/seq_relationship/output_weights',\n",
       " 'global_step'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(checkpoint_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vars = set()\n",
    "for v in bert_model.variables:\n",
    "    model_vars.add(v.name.split(':')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bert/embeddings/LayerNorm/beta',\n",
       " 'bert/embeddings/LayerNorm/gamma',\n",
       " 'bert/embeddings/position_embeddings',\n",
       " 'bert/embeddings/token_type_embeddings',\n",
       " 'bert/embeddings/word_embeddings',\n",
       " 'bert/pooler/dense/bias',\n",
       " 'bert/pooler/dense/kernel'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'bert/embeddings/token_type_embeddings:0' shape=(2, 768) dtype=float32, numpy=\n",
       " array([[-0.02514946,  0.01026926, -0.02847951, ...,  0.01424474,\n",
       "         -0.03260387, -0.01046715],\n",
       "        [ 0.00264451,  0.0291022 ,  0.01187141, ...,  0.00852287,\n",
       "          0.00036667, -0.00320941]], dtype=float32)>,\n",
       " <tf.Variable 'bert/embeddings/position_embeddings:0' shape=(512, 768) dtype=float32, numpy=\n",
       " array([[-0.00683917,  0.01018448, -0.03188806, ..., -0.02704441,\n",
       "          0.0003404 ,  0.01949455],\n",
       "        [ 0.00346667,  0.0070805 ,  0.00665813, ...,  0.02054542,\n",
       "         -0.003881  , -0.01862706],\n",
       "        [-0.00774869, -0.01833956, -0.00218828, ...,  0.02550897,\n",
       "         -0.01412613,  0.00711717],\n",
       "        ...,\n",
       "        [ 0.00097339,  0.00105322, -0.01415684, ..., -0.00699323,\n",
       "          0.00946521, -0.0128039 ],\n",
       "        [-0.00629447, -0.01220857,  0.03450704, ...,  0.00660803,\n",
       "         -0.00150067,  0.01885889],\n",
       "        [-0.01116214, -0.0172731 ,  0.02173424, ..., -0.01009039,\n",
       "         -0.00029322,  0.00046106]], dtype=float32)>,\n",
       " <tf.Variable 'bert/embeddings/word_embeddings:0' shape=(119547, 768) dtype=float32, numpy=\n",
       " array([[ 0.01826143,  0.03896691,  0.0088553 , ...,  0.02694518,\n",
       "         -0.01609657, -0.01999252],\n",
       "        [-0.02818528, -0.01265654,  0.01079152, ..., -0.01200113,\n",
       "          0.02340759, -0.00997289],\n",
       "        [ 0.00360482, -0.01625708, -0.01470868, ..., -0.02622728,\n",
       "         -0.0009144 , -0.00464051],\n",
       "        ...,\n",
       "        [ 0.00478843, -0.01749577,  0.03465902, ...,  0.00722118,\n",
       "         -0.01330613,  0.01482976],\n",
       "        [ 0.00667615,  0.01725923,  0.03533983, ...,  0.00728436,\n",
       "         -0.01853962, -0.01225242],\n",
       "        [ 0.01160367, -0.02454555,  0.02032644, ...,  0.0373105 ,\n",
       "          0.00098248,  0.00743059]], dtype=float32)>,\n",
       " <tf.Variable 'bert/embeddings/LayerNorm/gamma:0' shape=(768,) dtype=float32, numpy=\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1.], dtype=float32)>,\n",
       " <tf.Variable 'bert/embeddings/LayerNorm/beta:0' shape=(768,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'bert/pooler/dense/kernel:0' shape=(768, 768) dtype=float32, numpy=\n",
       " array([[ 0.00749065,  0.04849017, -0.04133564, ..., -0.01884747,\n",
       "          0.00764669,  0.0261755 ],\n",
       "        [ 0.02695347,  0.02953085,  0.00161618, ..., -0.02551788,\n",
       "         -0.05005389,  0.0321684 ],\n",
       "        [-0.05824701,  0.04342802,  0.04198864, ..., -0.06080776,\n",
       "          0.0060225 ,  0.04918423],\n",
       "        ...,\n",
       "        [ 0.02352934, -0.04888365,  0.01948242, ..., -0.04415841,\n",
       "          0.04389311, -0.02422231],\n",
       "        [ 0.05858976,  0.02313398, -0.03389345, ...,  0.05197576,\n",
       "         -0.05089565, -0.04075053],\n",
       "        [ 0.04573408,  0.00836277,  0.0173395 , ...,  0.05466998,\n",
       "         -0.0396392 ,  0.06112584]], dtype=float32)>,\n",
       " <tf.Variable 'bert/pooler/dense/bias:0' shape=(768,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py:1604: NameBasedSaverStatus.__init__ (from tensorflow.python.training.checkpointable.util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Some objects had attributes which were not restored: {<bert_dp.embeddings.BERTCombinedEmbedding object at 0x7f5ab43489b0>: ['OBJECT_CONFIG_JSON'], <tensorflow.python.keras.layers.core.Dropout object at 0x7f5a880519e8>: ['OBJECT_CONFIG_JSON'], <bert_dp.normalization.LayerNormalization object at 0x7f5a88051fd0>: ['OBJECT_CONFIG_JSON'], <bert_dp.encoder.TransformerEncoder object at 0x7f5a88051f60>: ['OBJECT_CONFIG_JSON'], <tensorflow.python.keras.layers.core.Dense object at 0x7f5a807ed240>: ['OBJECT_CONFIG_JSON']}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e40a709511d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0;31m# streaming restore for any variables created in the future.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1499\u001b[0m         \u001b[0mcheckpointable_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1500\u001b[0;31m       \u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_nontrivial_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1501\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py\u001b[0m in \u001b[0;36massert_nontrivial_match\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0;31m# assert_nontrivial_match and assert_consumed (and both are less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;31m# useful since we don't touch Python objects or Python state).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_consumed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_gather_saveable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/training/checkpointable/util.py\u001b[0m in \u001b[0;36massert_consumed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1232\u001b[0m       raise AssertionError(\n\u001b[1;32m   1233\u001b[0m           \u001b[0;34m\"Some objects had attributes which were not restored: %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1234\u001b[0;31m           % (unused_attributes,))\n\u001b[0m\u001b[1;32m   1235\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcheckpointable\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_root_checkpointable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Some objects had attributes which were not restored: {<bert_dp.embeddings.BERTCombinedEmbedding object at 0x7f5ab43489b0>: ['OBJECT_CONFIG_JSON'], <tensorflow.python.keras.layers.core.Dropout object at 0x7f5a880519e8>: ['OBJECT_CONFIG_JSON'], <bert_dp.normalization.LayerNormalization object at 0x7f5a88051fd0>: ['OBJECT_CONFIG_JSON'], <bert_dp.encoder.TransformerEncoder object at 0x7f5a88051f60>: ['OBJECT_CONFIG_JSON'], <tensorflow.python.keras.layers.core.Dense object at 0x7f5a807ed240>: ['OBJECT_CONFIG_JSON']}"
     ]
    }
   ],
   "source": [
    "bert_model.load_weights(init_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
