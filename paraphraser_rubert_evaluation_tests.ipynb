{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# uncomment preferred option\n",
    "#!pip install -q git+https://github.com/deepmipt/bert.git@feat/keras deeppavlov tensorflow-gpu==1.14.0\n",
    "#!pip install -q git+https://github.com/deepmipt/bert.git@feat/keras deeppavlov tensorflow-gpu==2.0.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "v = str(tf.__version__).split('.')[:2]\n",
    "assert int(v[0]) > 1 or (int(v[1]) >= 13), 'this notebook was tested with TF v1.14.0 and v2.0.0-beta1'\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "is_eager = tf.compat.v1.executing_eagerly()\n",
    "is_eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_dp import bert\n",
    "from bert_dp.metrics import F1Score\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./models/paraphraser_rubert_v0.tar.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.get_file(\n",
    "    fname='paraphraser_rubert_v0.tar.gz',\n",
    "    origin='http://files.deeppavlov.ai/deeppavlov_data/bert/paraphraser_rubert_v0.tar.gz',\n",
    "    cache_subdir='models',\n",
    "    extract=True,\n",
    "    cache_dir='.'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference model and data from DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_readers.paraphraser_reader import ParaphraserReader\n",
    "ds = ParaphraserReader().read(data_path='dataset', do_lower_case=False)\n",
    "texts_a = []\n",
    "texts_b = []\n",
    "labels = []\n",
    "for (ta, tb), label in ds['test']:\n",
    "    texts_a.append(ta)\n",
    "    texts_b.append(tb)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-21 14:22:28.839 INFO in 'deeppavlov.download'['download'] at line 116: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz download because of matching hashes\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "I0621 14:22:28.839187 139961005266752 download.py:116] Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v1.tar.gz download because of matching hashes\n",
      "2019-06-21 14:22:29.752 INFO in 'deeppavlov.download'['download'] at line 116: Skipped http://files.deeppavlov.ai/deeppavlov_data/classifiers/paraphraser_rubert_v0.tar.gz download because of matching hashes\n",
      "I0621 14:22:29.752000 139961005266752 download.py:116] Skipped http://files.deeppavlov.ai/deeppavlov_data/classifiers/paraphraser_rubert_v0.tar.gz download because of matching hashes\n",
      "2019-06-21 14:22:29.767 INFO in 'deeppavlov.download'['download'] at line 116: Skipped http://files.deeppavlov.ai/datasets/paraphraser.zip download because of matching hashes\n",
      "I0621 14:22:29.767950 139961005266752 download.py:116] Skipped http://files.deeppavlov.ai/datasets/paraphraser.zip download because of matching hashes\n",
      "2019-06-21 14:22:29.775 INFO in 'deeppavlov.download'['download'] at line 116: Skipped http://files.deeppavlov.ai/datasets/paraphraser_gold.zip download because of matching hashes\n",
      "I0621 14:22:29.775826 139961005266752 download.py:116] Skipped http://files.deeppavlov.ai/datasets/paraphraser_gold.zip download because of matching hashes\n",
      "[nltk_data] Downloading package punkt to /home/nab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/nab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to /home/nab/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/nab/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "W0621 14:22:34.751618 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/core/models/tf_model.py:38: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0621 14:22:34.752649 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/core/models/tf_model.py:223: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0621 14:22:34.753110 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/core/models/tf_model.py:223: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Using TensorFlow backend.\n",
      "W0621 14:22:34.791175 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/core/models/tf_model.py:194: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "W0621 14:22:34.797018 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/bert/bert_dp/modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W0621 14:22:34.797753 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/models/bert/bert_classifier.py:84: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0621 14:22:34.967281 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/models/bert/bert_classifier.py:155: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0621 14:22:34.974415 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/bert/bert_dp/embeddings.py:63: The name tf.keras.initializers.TruncatedNormal is deprecated. Please use tf.compat.v1.keras.initializers.TruncatedNormal instead.\n",
      "\n",
      "W0621 14:22:34.975161 139961005266752 deprecation.py:506] From /home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:94: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0621 14:22:34.984599 139961005266752 deprecation.py:506] From /home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0621 14:22:38.286988 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/models/bert/bert_classifier.py:117: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0621 14:22:38.294669 139961005266752 deprecation.py:506] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/models/bert/bert_classifier.py:125: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0621 14:22:48.311695 139961005266752 deprecation.py:323] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/models/bert/bert_classifier.py:97: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "2019-06-21 14:22:48.313 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 52: [loading model from /home/nab/.deeppavlov/models/paraphraser_rubert/model_rubert]\n",
      "I0621 14:22:48.313849 139961005266752 tf_model.py:52] [loading model from /home/nab/.deeppavlov/models/paraphraser_rubert/model_rubert]\n",
      "W0621 14:22:48.315916 139961005266752 deprecation_wrapper.py:119] From /home/nab/PycharmProjects/work/DeepPavlov/deeppavlov/core/models/tf_model.py:55: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov import build_model, configs\n",
    "\n",
    "para_model = build_model(configs.classifiers.paraphraser_rubert, download=True)\n",
    "\n",
    "bert_preprocessor = para_model[0]\n",
    "bert_classifier = para_model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "features_array = bert_preprocessor(texts_a, texts_b)\n",
    "x_test = np.array([f.input_ids for f in features_array])\n",
    "y_test = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p = bert_classifier(features_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.keras.Model construction and compilation for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifierCompat(tf.keras.Model):  # bert.BERT):\n",
    "    \"\"\"Subclassed model for classification compatible with official checkpoints from Google.\"\"\"\n",
    "    def __init__(self, num_classes=2, **kwargs):\n",
    "        kwargs['name'] = 'bert'\n",
    "#         with tf.name_scope('bert'):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.bert = bert.BERT(name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.5)\n",
    "        self.softmax = tf.keras.layers.Activation(activation='softmax')\n",
    "        \n",
    "    def build(self, batch_input_shape):\n",
    "        self.output_weights = self.add_weight(shape=(self.num_classes, 768),\n",
    "                                              dtype=tf.float32,\n",
    "                                              initializer=tf.keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                                              name=\"output_weights\")\n",
    "        self.output_bias = self.add_weight(shape=(self.num_classes),\n",
    "                                           dtype=tf.float32,\n",
    "                                           initializer=tf.keras.initializers.Zeros(),\n",
    "                                           name='output_bias')\n",
    "        super().build(batch_input_shape)\n",
    "        \n",
    "    def call(self, token_ids, training=None, mask=None, **kwargs):\n",
    "        po = self.bert(token_ids, training=training, mask=mask)\n",
    "        po = self.dropout(po, training=training)\n",
    "        int_logits = tf.matmul(po, self.output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(int_logits, self.output_bias)\n",
    "        out = self.softmax(logits)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "paraphrase_detector = BERTClassifierCompat()\n",
    "# we build the model here rather than simply calling it in order to properly recreate variable names\n",
    "paraphrase_detector.build(batch_input_shape=(BATCH_SIZE, None))\n",
    "r1 = paraphrase_detector.predict(x_test[:BATCH_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 14:24:00.229381 139961005266752 deprecation.py:506] From /home/nab/PycharmProjects/work/venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:507: calling count_nonzero (from tensorflow.python.ops.math_ops) with axis is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "reduction_indices is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-05\n",
    "paraphrase_detector.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE,\n",
    "                                                               epsilon=1e-6),\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "                            metrics=[tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "                                     F1Score(num_classes=2)\n",
    "                                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899/1899 [==============================] - 10s 5ms/sample - loss: 0.6867 - sparse_categorical_accuracy: 0.5540 - f1_score: 0.6972\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.68670883161134, 0.55397576, 0.69717556]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline to ensure checkpoint loading make a difference\n",
    "paraphrase_detector.evaluate(x=x_test, y=y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0621 14:24:16.976696 139961005266752 saver.py:795] Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\n"
     ]
    }
   ],
   "source": [
    "paraphraser_checkpoint_path = 'models/paraphraser_rubert/model_rubert'\n",
    "paraphraser_saver = tf.compat.v1.train.Saver(var_list=paraphrase_detector.variables)\n",
    "paraphraser_saver.restore(sess=None if is_eager else tf.keras.backend.get_session(),\n",
    "                          save_path=paraphraser_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are changed\n"
     ]
    }
   ],
   "source": [
    "r2 = paraphrase_detector.predict(x_test[:BATCH_SIZE])\n",
    "try:\n",
    "    np.testing.assert_allclose(r1, r2)\n",
    "except AssertionError:\n",
    "    # check at least some variables were loaded\n",
    "    print('Outputs are changed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899/1899 [==============================] - 9s 4ms/sample - loss: 0.3883 - sparse_categorical_accuracy: 0.8489 - f1_score: 0.8789\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.38825273774309244, 0.84886783, 0.87885183]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paraphrase_detector.evaluate(x=x_test, y=y_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the RuBERT paper: accuracy = 84.99 +- 0.35, f1 = 87.73 +- 0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ = np.argmax(paraphrase_detector.predict(x_test), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(p_ == p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
